Paste this content:

```markdown
# AI Visibility Lifecycle Specification (v0.7)

> ⚠️ **This document is NOT the canonical source.**
> The authoritative reference is the Zenodo deposit: https://doi.org/10.5281/zenodo.18460711
> This specification mirrors the canonical source for developer accessibility. In case of discrepancy, Zenodo governs.

---

## Overview

The 11-Stage AI Visibility Lifecycle is a stage-based observational framework describing how digital content achieves visibility within AI discovery, comprehension, trust, and human exposure systems.

## The Three Phases

**Stages 1–5: AI Comprehension** — AI systems discover, interpret, classify, harmonize, and validate the domain's knowledge structure against global sources.

**Stages 6–8: Trust Establishment** — AI systems build longitudinal trust, formally accept the domain as reliable, and determine competitive readiness for human surfacing.

**Stages 9–11: Human Visibility** — AI systems cautiously test the domain with real users, establish baseline ranking, and scale visibility based on sustained positive performance.

---

## Stage Definitions

### Stage 1 — AI Crawling
AI systems discover the domain through URL submissions, sitemaps, beacons, inter-domain signals, or autonomous exploration. Pages are fetched, rendered, and prepared for semantic analysis. This is pure discovery and reconnaissance—no interpretation or trust exists yet.

### Stage 2 — AI Ingestion
Raw content is decomposed into tokens, parsed for structure, and transformed into semantic embeddings. AI extracts ontologies, generates vector representations, and creates a provisional knowledge graph. The domain's content becomes machine-readable semantic material.

### Stage 3 — AI Classification (Purpose & Identity Assignment)
AI determines what kind of website it is dealing with: educational, commercial, institutional, advisory, or hybrid. This classification governs every downstream process—including safety thresholds, risk levels, ranking potential, and the strictness of evaluation. Purpose clarity is essential; ambiguity slows progression.

### Stage 4 — AI Harmony Checks (Internal Consistency Evaluation)
AI checks whether the website is internally coherent: consistent structure, tone, definitions, intent, and schema across all pages. Pages must "agree with each other" conceptually and structurally. This phase eliminates chaotic, contradictory, or low-coherence domains early.

### Stage 5 — AI Cross-Correlation (External Alignment Verification)
AI checks whether the site's content aligns with external, globally verified knowledge sources: government databases, foundational references, high-authority educational bodies, scientific repositories, occupational frameworks. AI is assessing: "Does this site fit into the global consensus?" High alignment → trust potential.

### Stage 6 — AI Trust Building (Accumulating Evidence Over Time)
AI gathers evidence of reliability across multiple layers: long-term stability, accuracy, consistency, neutrality, structural integrity, and purpose transparency. Trust is iterative, not binary—AI must see repeated proof over many crawls. Only sites with durable integrity progress.

### Stage 7 — AI Trust Acceptance (Formal Eligibility for Use in Answers)
Once trust signals cross a threshold, AI formally marks the domain as a reliable reference node. It becomes eligible for use in answer synthesis, citations, and multi-source reasoning. The domain now exists in the AI's "trusted knowledge set," but is not yet visible to humans.

### Stage 8 — Candidate Surfacing (Competitive Readiness Assessment)
AI evaluates whether a trusted domain should enter the human-facing competitive layer. It maps query relevance, benchmarks against visible competitors, scores user-value potential, and tests visibility risk. This determines when and where the domain becomes eligible for human exposure.

### Stage 9 — Early Human Visibility Testing (Controlled User Experiments)
AI exposes the domain to a tiny fraction of real search queries (<0.1% traffic) and measures user behavior: satisfaction, dwell time, task completion, return rates. This validates whether real humans find the content useful. Poor performance pauses progression; strong performance advances to Stage 10.

### Stage 10 — Baseline Human Ranking (First Stable Search Placement)
The site is now included in real SERPs in a controlled, low-risk fashion—typically for long-tail and mid-tail queries. AI measures behavior at scale, compares outcomes against competitors, and checks regional stability. This stage establishes the first reliable human traffic baseline.

### Stage 11 — Growth Visibility & Human Traffic Acceleration
If baseline performance is strong, AI expands visibility across regions, query families, device types, and tail depths. Human traffic increases meaningfully and predictably. The domain enters the global search ecosystem as a scalable, reliable knowledge asset.

---

## Key Principles

1. Stages 1-2 are sequential gates; Stages 3-11 operate as parallel evaluation dimensions.
2. Architectural quality determines timeline; commercial classification determines trust threshold height.
3. Crawlability (Stage 1) ≠ Visibility (Stages 9-11).
4. Baseline success rate: 1-6% for all websites. Projected rate: 50-70% for optimized sites (analytical estimates).

---

## Canonical Source

**Zenodo:** https://doi.org/10.5281/zenodo.18460711

Related: [cv4students.com](https://cv4students.com) | [aivisibilityarchitects.com](https://aivisibilityarchitects.com)

## Author

Bernard Lynch
AI Visibility Architecture Group Limited, Auckland, New Zealand

## License

CC BY-NC-ND 4.0

You are free to share this work with attribution, but you may not use it for commercial purposes or distribute modified versions.
```

Then click **Commit changes**.
